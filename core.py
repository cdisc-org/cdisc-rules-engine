import asyncio
import json
import logging
import os
import pickle
import tempfile
from datetime import datetime
from multiprocessing import freeze_support
from typing import Tuple

import click
from pathlib import Path
from cdisc_rules_engine.config import config
from cdisc_rules_engine.enums.default_file_paths import DefaultFilePaths
from cdisc_rules_engine.enums.progress_parameter_options import ProgressParameterOptions
from cdisc_rules_engine.enums.report_types import ReportTypes
from cdisc_rules_engine.enums.dataformat_types import DataFormatTypes
from cdisc_rules_engine.models.validation_args import Validation_args
from scripts.run_validation import run_validation
from cdisc_rules_engine.services.cache.cache_populator_service import CachePopulator
from cdisc_rules_engine.services.cache.cache_service_factory import CacheServiceFactory
from cdisc_rules_engine.services.cdisc_library_service import CDISCLibraryService
from cdisc_rules_engine.models.external_dictionaries_container import (
    ExternalDictionariesContainer,
    DictionaryTypes,
)
from cdisc_rules_engine.utilities.utils import (
    generate_report_filename,
    get_rules_cache_key,
)
from scripts.list_dataset_metadata_handler import list_dataset_metadata_handler
from version import __version__


def valid_data_file(data_path: list) -> Tuple[list, set]:
    allowed_formats = [format.value for format in DataFormatTypes]
    found_formats = set()
    file_list = []
    for file in data_path:
        file_extension = os.path.splitext(file)[1][1:].upper()
        if file_extension in allowed_formats:
            found_formats.add(file_extension)
            file_list.append(file)
    if len(found_formats) > 1:
        return [], found_formats
    elif len(found_formats) == 1:
        return file_list, found_formats


@click.group()
def cli():
    pass


@click.command()
@click.option(
    "-ca",
    "--cache",
    default=DefaultFilePaths.CACHE.value,
    help="Relative path to cache files containing pre loaded metadata and rules",
)
@click.option(
    "-ps",
    "--pool-size",
    default=10,
    type=int,
    help="Number of parallel processes for validation",
)
@click.option(
    "-d",
    "--data",
    required=False,
    help="Path to directory containing data files",
)
@click.option(
    "-dp",
    "--dataset-path",
    required=False,
    multiple=True,
    help="Absolute path to dataset file",
)
@click.option(
    "-l",
    "--log-level",
    default="disabled",
    type=click.Choice(["info", "debug", "error", "critical", "disabled", "warn"]),
    help="Sets log level for engine logs, logs are disabled by default",
)
@click.option(
    "-rt",
    "--report-template",
    default=DefaultFilePaths.EXCEL_TEMPLATE_FILE.value,
    help="File path of report template to use for excel output",
)
@click.option(
    "-s",
    "--standard",
    required=True,
    default=None,
    help="CDISC standard to validate against",
)
@click.option(
    "-v",
    "--version",
    required=True,
    default=None,
    help="Standard version to validate against",
)
@click.option(
    "-ss",
    "--substandard",
    default=None,
    help="CDISC Substandard to validate against",
)
@click.option(
    "-ct",
    "--controlled-terminology-package",
    multiple=True,
    help=(
        "Controlled terminology package to validate against, "
        "can provide more than one"
    ),
)
@click.option(
    "-o",
    "--output",
    default=generate_report_filename(datetime.now().isoformat()),
    help="Report output file destination",
)
@click.option(
    "-of",
    "--output-format",
    multiple=True,
    default=[ReportTypes.XLSX.value],
    type=click.Choice(ReportTypes.values(), case_sensitive=False),
    help="Output file format",
)
@click.option(
    "-rr",
    "--raw-report",
    default=False,
    show_default=True,
    is_flag=True,
    help=(
        "Report in a raw format as it is generated by the engine. "
        "This flag must be used only with --output-format JSON."
    ),
)
@click.option(
    "-dv",
    "--define-version",
    type=click.Choice(["2-1", "2-0", "2.0", "2.1"]),
    help="Define-XML version used for validation",
)
@click.option("--whodrug", help="Path to directory with WHODrug dictionary files")
@click.option("--meddra", help="Path to directory with MedDRA dictionary files")
@click.option("--loinc", help="Path to directory with LOINC dictionary files")
@click.option("--medrt", help="Path to directory with MEDRT dictionary files")
@click.option("--unii", help="Path to directory with UNII dictionary files")
@click.option("--snomed-version", help="Version of snomed to use.")
@click.option(
    "--snomed-url",
    help="The Base URL of snomed to use. Defaults to snowstorm test instance",
    default="https://snowstorm.snomedtools.org/snowstorm/snomed-ct/",
)
@click.option("--snomed-edition", help="Edition of snomed to use.")
@click.option(
    "--rules",
    "-r",
    multiple=True,
    help="specify rule core ID ex. CORE-000001. Can be specified multiple times",
)
@click.option(
    "--local_rules",
    "-lr",
    required=False,
    type=click.Path(exists=True, readable=True, resolve_path=True),
    help="path to directory containing local rules.",
)
@click.option(
    "--custom_standard",
    "-cs",
    required=False,
    is_flag=True,
    default=False,
    help=("flag to run a validation using a custom_standard from the cache"),
)
@click.option(
    "-p",
    "--progress",
    default=ProgressParameterOptions.BAR.value,
    type=click.Choice(ProgressParameterOptions.values()),
    help=(
        "Defines how to display the validation progress. "
        'By default a progress bar like "[████████████████████████████--------]   78%"'
        "is printed."
    ),
)
@click.option("-dxp", "--define-xml-path", required=False, help="Path to Define-XML")
@click.option(
    "-vx",
    "--validate-xml",
    default="y",
    help="Enable XML validation (default 'y' to enable, otherwise disable)",
)
@click.pass_context
def validate(
    ctx,
    cache: str,
    pool_size: int,
    data: str,
    dataset_path: Tuple[str],
    log_level: str,
    report_template: str,
    standard: str,
    version: str,
    substandard: str,
    controlled_terminology_package: Tuple[str],
    output: str,
    output_format: Tuple[str],
    raw_report: bool,
    define_version: str,
    whodrug: str,
    meddra: str,
    loinc: str,
    medrt: str,
    unii: str,
    snomed_version: str,
    snomed_edition: str,
    snomed_url: str,
    rules: Tuple[str],
    local_rules: str,
    custom_standard: bool,
    progress: str,
    define_xml_path: str,
    validate_xml: str,
):
    """
    Validate data using CDISC Rules Engine

    Example:

    python core.py -s SDTM -v 3.4 -d /path/to/datasets
    """

    # Validate conditional options
    logger = logging.getLogger("validator")

    if raw_report is True:
        if not (len(output_format) == 1 and output_format[0] == ReportTypes.JSON.value):
            logger.error(
                "Flag --raw-report can be used only when --output-format is JSON"
            )
            ctx.exit()

    cache_path: str = os.path.join(os.path.dirname(__file__), cache)

    # Construct ExternalDictionariesContainer:
    external_dictionaries = ExternalDictionariesContainer(
        {
            DictionaryTypes.UNII.value: unii,
            DictionaryTypes.MEDRT.value: medrt,
            DictionaryTypes.MEDDRA.value: meddra,
            DictionaryTypes.WHODRUG.value: whodrug,
            DictionaryTypes.LOINC.value: loinc,
            DictionaryTypes.SNOMED.value: {
                "edition": snomed_edition,
                "version": snomed_version,
                "base_url": snomed_url,
            },
        }
    )
    if data:
        if dataset_path:
            logger.error(
                "Argument --dataset-path cannot be used together with argument --data"
            )
            ctx.exit()
        dataset_paths, found_formats = valid_data_file(
            [str(Path(data).joinpath(fn)) for fn in os.listdir(data)]
        )
        if len(found_formats) > 1:
            logger.error(
                f"Argument --data contains more than one allowed file format ({', '.join(found_formats)})."  # noqa: E501
            )
            ctx.exit()
    elif dataset_path:
        dataset_paths, found_formats = valid_data_file([dp for dp in dataset_path])
        if len(found_formats) > 1:
            logger.error(
                f"Argument --dataset_path contains more than one allowed file format ({', '.join(found_formats)})."  # noqa: E501
            )
            ctx.exit()
    else:
        logger.error(
            "You must pass one of the following arguments: --dataset-path, --data"
        )
        # no need to define dataset_paths here, the program execution will stop
        ctx.exit()
    validate_xml_bool = True if validate_xml.lower() in ("y", "yes") else False
    run_validation(
        Validation_args(
            cache_path,
            pool_size,
            dataset_paths,
            log_level,
            report_template,
            standard,
            version,
            substandard,
            set(controlled_terminology_package),  # avoiding duplicates
            output,
            set(output_format),  # avoiding duplicates
            raw_report,
            define_version,
            external_dictionaries,
            rules,
            local_rules,
            custom_standard,
            progress,
            define_xml_path,
            validate_xml_bool,
        )
    )


@click.command()
@click.option(
    "-c",
    "--cache_path",
    default=DefaultFilePaths.CACHE.value,
    help="Relative path to cache files containing pre loaded metadata and rules",
)
@click.option(
    "--apikey",
    envvar="CDISC_LIBRARY_API_KEY",
    help=(
        "CDISC Library api key. "
        "Can be provided in the environment "
        "variable CDISC_LIBRARY_API_KEY"
    ),
    required=True,
)
@click.option(
    "-crd",
    "--custom_rules_directory",
    help=(
        "Relative path to directory containing local rules in yaml or JSON formats"
        "to be added to the cache. "
    ),
)
@click.option(
    "-cr",
    "--custom_rule",
    multiple=True,
    help=(
        "Relative path to rule file in yaml or JSON formats"
        "to be added to the cache. "
    ),
)
@click.option(
    "-rcr",
    "--remove_custom_rules",
    help=(
        "Remove rules from the cache. Can be a single rule ID, a comma-separated list of IDs, "
        "or 'ALL' to remove all custom rules."
    ),
)
@click.option(
    "-ucr",
    "--update_custom_rule",
    help=(
        "Relative path to rule file in yaml or JSON formats"
        "Rule will be updated in cache with this file. "
    ),
)
@click.option(
    "-cs",
    "--custom_standard",
    help=(
        "Relative path to JSON file containing custom standard details."
        "Will update the standard if it already exists."
    ),
)
@click.option(
    "-rcs",
    "--remove_custom_standard",
    help=("removes a custom standard and version from the cache. "),
    multiple=True,
)
@click.pass_context
def update_cache(
    ctx: click.Context,
    cache_path: str,
    apikey: str,
    custom_rules_directory: str,
    custom_rule: str,
    remove_custom_rules: str,
    update_custom_rule: str,
    custom_standard: str,
    remove_custom_standard: str,
):
    cache = CacheServiceFactory(config).get_cache_service()
    library_service = CDISCLibraryService(apikey, cache)
    cache_populator = CachePopulator(
        cache,
        library_service,
        custom_rules_directory,
        custom_rule,
        remove_custom_rules,
        update_custom_rule,
        custom_standard,
        remove_custom_standard,
        cache_path,
    )
    if custom_rule or custom_rules_directory:
        cache_populator.add_custom_rules()
    elif remove_custom_rules:
        cache_populator.remove_custom_rules_from_cache()
    elif update_custom_rule:
        cache_populator.update_custom_rule_in_cache()
    elif custom_standard:
        cache_populator.add_custom_standard_to_cache()
    elif remove_custom_standard:
        cache_populator.remove_custom_standards_from_cache()
    else:
        asyncio.run(cache_populator.update_cache())

    print("Cache updated successfully")


@click.command()
@click.option(
    "-c",
    "--cache_path",
    default=DefaultFilePaths.CACHE.value,
    help="Relative path to cache files containing pre loaded metadata and rules",
)
@click.option(
    "-s", "--standard", required=False, help="CDISC standard to get rules for"
)
@click.option(
    "-v", "--version", required=False, help="Standard version to get rules for"
)
@click.option(
    "-ss",
    "--substandard",
    required=False,
    default=None,
    help="CDISC substandard to get rules for. Any of SDTM, SEND, ADaM, CDASH",
)
@click.option(
    "-cr",
    "--custom_rules",
    is_flag=True,
    default=False,
    required=False,
    help="flag to list custom rules in the cache",
)
@click.option(
    "-r",
    "--rule_id",
    required=False,
    help="Rule ID to get rule for.",
    multiple=True,
)
@click.pass_context
def list_rules(
    ctx: click.Context,
    cache_path: str,
    standard: str,
    version: str,
    substandard: str,
    custom_rules: bool,
    rule_id: str,
):
    # Load all rules
    if custom_rules:
        rules_file = DefaultFilePaths.CUSTOM_RULES_CACHE_FILE.value
        dict_file = DefaultFilePaths.CUSTOM_RULES_DICTIONARY.value
    else:
        rules_file = DefaultFilePaths.RULES_CACHE_FILE.value
        dict_file = DefaultFilePaths.RULES_DICTIONARY.value
    with open(os.path.join(cache_path, rules_file), "rb") as f:
        rules_data = pickle.load(f)
    with open(os.path.join(cache_path, dict_file), "rb") as f:
        rules_dict = pickle.load(f)
    rules = []
    if rule_id:
        for id in rule_id:
            if id in rules_data:
                rules.append(rules_data[id])
    elif standard and version:
        key_prefix = get_rules_cache_key(
            standard, version.replace(".", "-"), substandard
        )
        if key_prefix in rules_dict:
            rule_ids = rules_dict[key_prefix]
            for rid in rule_ids:
                if rid in rules_data:
                    rules.append(rules_data[rid])
    else:
        # Print all rules
        rules = list(rules_data.values())
    print(json.dumps(rules, indent=4))


@click.command()
@click.option(
    "-c",
    "--cache_path",
    default=DefaultFilePaths.CACHE.value,
    help="Relative path to cache files containing pre loaded metadata and rules",
)
@click.pass_context
def list_rule_sets(ctx: click.Context, cache_path: str):
    """Lists all standards and versions for which rules are available."""
    rules_file = DefaultFilePaths.RULES_DICTIONARY.value
    with open(os.path.join(cache_path, rules_file), "rb") as f:
        rules_data = pickle.load(f)

    rule_sets = {}
    for key in rules_data.keys():
        if "/" in key:
            parts = key.split("/")
            standard = parts[0]
            version = parts[1]
            substandard = parts[2] if len(parts) > 2 else None
            if substandard:
                version_key = f"{version}/{substandard}"
            else:
                version_key = version
            if standard not in rule_sets:
                rule_sets[standard] = set()
            rule_sets[standard].add(version_key)

    for standard in sorted(rule_sets.keys()):
        versions = sorted(rule_sets[standard])
        for version in versions:
            print(f"{standard.upper()}, {version}")


@click.command()
@click.option(
    "-dp",
    "--dataset-path",
    required=True,
    multiple=True,
)
@click.pass_context
def list_dataset_metadata(ctx: click.Context, dataset_path: Tuple[str]):
    """
    Command that lists metadata of given datasets.

    Input:
        core.py list-ds-metadata -dp=path_1 -dp=path_2 -dp=path_3 ...
    Output:
        [
           {
              "domain":"AE",
              "filename":"ae.xpt",
              "full_path":"/Users/Aleksei_Furmenkov/PycharmProjects/cdisc-rules-engine/resources/data/ae.xpt",
              "file_size":"38000",
              "label":"Adverse Events",
              "modification_date":"2020-08-21T09:14:26"
           },
           {
              "domain":"EX",
              "filename":"ex.xpt",
              "full_path":"/Users/Aleksei_Furmenkov/PycharmProjects/cdisc-rules-engine/resources/data/ex.xpt",
              "file_size":"78050",
              "label":"Exposure",
              "modification_date":"2021-09-17T09:23:22"
           },
           ...
        ]
    """
    print(json.dumps(list_dataset_metadata_handler(dataset_path), indent=4))


@click.command()
def version():
    print(__version__)


@click.command()
@click.option(
    "-c",
    "--cache_path",
    default=DefaultFilePaths.CACHE.value,
    help="Relative path to cache files containing pre loaded metadata and rules",
)
@click.option(
    "-s",
    "--subsets",
    help="CT package subset type. Ex: sdtmct. Multiple values allowed",
    required=False,
    multiple=True,
)
def list_ct(cache_path: str, subsets: Tuple[str]):
    """
    Command to list the ct packages available in the cache.
    """
    if subsets:
        subsets = set([subset.lower() for subset in subsets])

    for file in os.listdir(cache_path):
        file_prefix = file[0 : file.find("ct-") + 2]
        if file_prefix.endswith("ct") and (not subsets or file_prefix in subsets):
            print(os.path.splitext(file)[0])


@click.command()
def test_validate():
    """**Release Test** validate command for executable."""
    try:
        import sys
        import os
        from cdisc_rules_engine.models.validation_args import Validation_args
        from cdisc_rules_engine.models.external_dictionaries_container import (
            ExternalDictionariesContainer,
        )
        from cdisc_rules_engine.enums.report_types import ReportTypes
        from cdisc_rules_engine.enums.progress_parameter_options import (
            ProgressParameterOptions,
        )
        from cdisc_rules_engine.enums.default_file_paths import DefaultFilePaths

        base_path = os.path.join("tests", "resources", "datasets")
        ts_path = os.path.join(base_path, "TS.json")
        ae_path = os.path.join(base_path, "ae.xpt")
        if not all(os.path.exists(path) for path in [ts_path, ae_path]):
            raise FileNotFoundError(
                "Test datasets not found in tests/resources/datasets"
            )

        with tempfile.TemporaryDirectory() as temp_dir:
            cache_path = DefaultFilePaths.CACHE.value
            pool_size = 10
            log_level = "disabled"
            report_template = DefaultFilePaths.EXCEL_TEMPLATE_FILE.value
            standard = "sdtmig"
            version = "3.4"
            substandard = None
            controlled_terminology_package = set()
            json_output = os.path.join(temp_dir, "json_validation_output")
            xpt_output = os.path.join(temp_dir, "xpt_validation_output")
            output_format = {ReportTypes.XLSX.value}
            raw_report = False
            define_version = None
            external_dictionaries = ExternalDictionariesContainer({})
            rules = []
            local_rules = None
            custom_standard = False
            progress = ProgressParameterOptions.BAR.value
            define_xml_path = None
            validate_xml = False
            json_output = os.path.join(temp_dir, "json_validation_output")
            run_validation(
                Validation_args(
                    cache_path,
                    pool_size,
                    [ts_path],
                    log_level,
                    report_template,
                    standard,
                    version,
                    substandard,
                    controlled_terminology_package,
                    json_output,
                    output_format,
                    raw_report,
                    define_version,
                    external_dictionaries,
                    rules,
                    local_rules,
                    custom_standard,
                    progress,
                    define_xml_path,
                    validate_xml,
                )
            )
            print("JSON validation completed successfully!")
            xpt_output = os.path.join(temp_dir, "xpt_validation_output")
            run_validation(
                Validation_args(
                    cache_path,
                    pool_size,
                    [ae_path],
                    log_level,
                    report_template,
                    standard,
                    version,
                    substandard,
                    controlled_terminology_package,
                    xpt_output,
                    output_format,
                    raw_report,
                    define_version,
                    external_dictionaries,
                    rules,
                    local_rules,
                    custom_standard,
                    progress,
                    define_xml_path,
                    validate_xml,
                )
            )
            print("XPT validation completed successfully!")
        print("All validation tests completed successfully!")
        sys.exit(0)
    except Exception as e:
        print(f"Validation test failed: {str(e)}")
        sys.exit(1)


cli.add_command(test_validate)
cli.add_command(validate)
cli.add_command(update_cache)
cli.add_command(list_rules)
cli.add_command(list_rule_sets)
cli.add_command(list_dataset_metadata)
cli.add_command(version)
cli.add_command(list_ct)

if __name__ == "__main__":
    freeze_support()
    cli()
